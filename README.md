
üìå Project Overview

 This project demonstrates the practical application of core data engineering tools and concepts by simulating a real-world, high-throughput data environment. The simulation generates data at a rate of 120 records per minute‚Äîequivalent to approximately 50 million records per year‚Äîusing Apache Kafka as the data ingestion layer.

The data pipeline operates in two stages:

Historical Data Load ‚Äì Ingesting backlog data from previous years.

Real-Time Data Stream ‚Äì Continuously capturing newly generated records.

Both datasets are sourced from a central database table that stores client information for backend APIs. This stage also includes data cleaning and transformation logic to ensure consistency and quality.

The processed and normalized data is then loaded into Google BigQuery, serving as the project‚Äôs data warehouse. From there, it connects to an interactive dashboard for analytics and visualization.

For demonstration purposes, the project focuses on identifying peak commercial activity periods per item across brands onboarded on Selar, highlighting insights into time-based performance trends.

üîç Objectives

Building a Scalable engineering process that normalised data extracted off daily transactional record with very affordable cost whilst maintaining engineering principles, data persistence, and overall data maintenance quality.

Engineering Flow
  This pipeline was built bearing in mind that the primary source of data was going to be coming from daily commercial transactions, inclusive of elements like :
    -Brand
    -Customer Name
    -Quantity of Item Purchased
    -Total Cost
    -Time of purchase
    -Nature of Delivery
    -Nature of Pay etc.
  All elements here points to typical Online Transaction Processing (OLTP) databases, meaning a Cloud Sql instance or data repositor of similar nature already exists for storing daily data, meaning a virtual machine instance also exists for data processing.
  To achieve subsidized cost and normalising data-backlogs whilst creating a befitting architecture to processing incoming data, we would be using these tools:
    -Airflow : for task ochestration.
    -Postgres : For temporarily staging data.
    -DBT : For modularized querying, upon a window-sized dataset on daily intervals.
    -Big Query: For data-warehousing, a sink for your analytic dashboard (looker studio etc)
    -Kafka : For data Simulation, automating data to test-run architecture with similar 50million records/year through put.
  Kafka gets powered to feed data into the central OLTP database, simulating a real-world scenario, and a scheduler triggers airflow dags at interval calling all down-stream tasks in the proper lineage for data order and proper aggregation. Store raw data in Google cloud Storage. Normalize and Partition the data by date in bigQuery data warehouse. Load processed data into BigQuery for analysis. Create dbt models to further draw insights in automated fashion from the temporal daily persisted dataset in postgres CLOUD SQL. Create SQL queries and dashboards to generate insights on looker studio. 

COST ANALYSIS:
  - We'd be leveraging VM instance used for daily transactional data persistence for running incremental daily querying generated by users - This automatically takes out excessive compute cost for data warehousing.
  - If we currently have a large enough VM instance used for transactional data persistence, we can aswell deploy our orchestrator on the VM if not, we utilize a mini gc-2 instance for hosting all pipeline depenedent tools - 30 dollars per month
  - We would use big query for dashboard analysis while all downstream querying would occur on dbt inside the cloud sql incurring 0 charge.
    - Big Query cahrges about 0.02 dollars per Gb per Month. A 50 million row dataset would likely be around ~20Gb, meaning 0.4 dollars per month.
    - Big Query Charges $0.006 per GB or $0.000006 per MB, so if we have about 10 dashboards feeding off each normalised chart, and each query is cost effective and clean enough not to accrue much cost we would be comfortably sitting under 5 ~8 dollars per month.
  This keeps total cost to be about 40 dollars per month



