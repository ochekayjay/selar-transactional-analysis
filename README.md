
üìå Project Overview

 This is a simple project that utilizes necessary data-engineering leaning tools by simulating a real-world scenario with data being automatically manufactured at the speed of 120 records every minute to match the 50million/year record in practice with kafka. The data fed into the pipeline is in two phases one from the backlog of data from previous years, the other is from currently generated data. Both data is extracted off the central database table used for persisting client data for backend apis, and all cleaning and transforming queries would also go in here. Afterwards the normalised dataset gets fed into the big query data-warehouse which is chained to a dashboard. For the puprose of illustration, i built a simple project which tracks the busiest time of commercial activity per item for each brand onboarded on selar.

üîç Objectives

Building a Scalable engineering process that normalised data extracted off daily transactional record with very affordable cost whilst maintaining engineering principles, data persistence, and overall data maintenance quality.

Engineering Flow
  This pipeline was built bearing in mind that the primary source of data was going to be coming from daily commercial transactions, inclusive of elements like :
    -Brand
    -Customer Name
    -Quantity of Item Purchased
    -Total Cost
    -Time of purchase
    -Nature of Delivery
    -Nature of Pay etc.
  All elements here points to typical Online Transaction Processing (OLTP) databases, meaning a Cloud Sql instance or data repositor of similar nature already exists for storing daily data, meaning a virtual machine instance also exists for data processing.
  To achieve subsidized cost and normalising data-backlogs whilst creating a befitting architecture to processing incoming data, we would be using these tools:
    -Airflow : for task ochestration.
    -Postgres : For temporarily staging data.
    -DBT : For modularized querying, upon a window-sized dataset on daily intervals.
    -Big Query: For data-warehousing, a sink for your analytic dashboard (looker studio etc)
    -Kafka : For data Simulation, automating data to test-run architecture with similar 50million records/year through put.
  Kafka gets powered to feed data into the central OLTP database, simulating a real-world scenario, and a scheduler triggers airflow dags at interval calling all down-stream tasks in the proper lineage for data order and proper aggregation. Store raw data in Google cloud Storage. Normalize and Partition the data by date in bigQuery data warehouse. Load processed data into BigQuery for analysis. Create dbt models to further draw insights in automated fashion from the temporal daily persisted dataset in postgres CLOUD SQL. Create SQL queries and dashboards to generate insights on looker studio. 

COST ANALYSIS:
  - We'd be leveraging VM instance used for daily transactional data persistence for running incremental daily querying generated by users - This automatically takes out excessive compute cost for data warehousing.
  - If we currently have a large enough VM instance used for transactional data persistence, we can aswell deploy our orchestrator on the VM if not, we utilize a mini gc-2 instance for hosting all pipeline depenedent tools - 30 dollars per month
  - We would use big query for dashboard analysis while all downstream querying would occur on dbt inside the cloud sql incurring 0 charge.
    - Big Query cahrges about 0.02 dollars per Gb per Month. A 50 million row dataset would likely be around ~20Gb, meaning 0.4 dollars per month.
    - Big Query Charges $0.006 per GB or $0.000006 per MB, so if we have about 10 dashboards feeding off each normalised chart, and each query is cost effective and clean enough not to accrue much cost we would be comfortably sitting under 5 ~8 dollars per month.
  This keeps total cost to be about 40 dollars per month



Partitioning: Based on flight date to improve query performance.

About Batch Processing Project
