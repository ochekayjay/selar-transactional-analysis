
üìå Project Overview

 This project demonstrates the practical application of core data engineering tools and concepts by simulating a real-world, high-throughput data environment. The simulation generates data at a rate of 120 records per minute‚Äîequivalent to approximately 50 million records per year‚Äîusing Apache Kafka as the data ingestion layer.

The data pipeline operates in two stages:

Historical Data Load ‚Äì Ingesting backlog data from previous years.

Real-Time Data Stream ‚Äì Continuously capturing newly generated records.

Both datasets are sourced from a central database table that stores client information for backend APIs. This stage also includes data cleaning and transformation logic to ensure consistency and quality.

The processed and normalized data is then loaded into Google BigQuery, serving as the project‚Äôs data warehouse. From there, it connects to an interactive dashboard for analytics and visualization.

For demonstration purposes, the project focuses on identifying peak commercial activity periods per item across brands onboarded on Selar, highlighting insights into time-based performance trends.

üîç Objectives

Building a Scalable engineering process that normalised data extracted off daily transactional record with very affordable cost whilst maintaining engineering principles, data persistence, and overall data maintenance quality.

Engineering Flow
  This pipeline was built bearing in mind that the primary source of data was going to be coming from daily commercial transactions, inclusive of elements like :
    -Brand
    -Customer Name
    -Quantity of Item Purchased
    -Total Cost
    -Time of purchase
    -Nature of Delivery
    -Nature of Pay etc.
  All elements here points to typical Online Transaction Processing (OLTP) databases, meaning a Cloud Sql instance or data repositor of similar nature already exists for storing daily data, meaning a virtual machine instance also exists for data processing.
  To achieve subsidized cost and normalising data-backlogs whilst creating a befitting architecture to processing incoming data, we would be using these tools:
    -Airflow : for task ochestration.
    -Postgres : For temporarily staging data.
    -DBT : For modularized querying, upon a window-sized dataset on daily intervals.
    -Big Query: For data-warehousing, a sink for your analytic dashboard (looker studio etc)
    -Kafka : For data Simulation, automating data to test-run architecture with similar 50million records/year through put.
  Kafka gets powered to feed data into the central OLTP database, simulating a real-world scenario, and a scheduler triggers airflow dags at interval calling all down-stream tasks in the proper lineage for data order and proper aggregation. Store raw data in Google cloud Storage. Normalize and Partition the data by date in bigQuery data warehouse. Load processed data into BigQuery for analysis. Create dbt models to further draw insights in automated fashion from the temporal daily persisted dataset in postgres CLOUD SQL. Create SQL queries and dashboards to generate insights on looker studio. 

COST ANALYSIS:
 The proposed setup is designed for cost efficiency by optimizing the use of existing infrastructure and minimizing additional cloud expenses.

- Compute Optimization:
   We leverage the same VM instance currently used for daily transactional data persistence to run incremental daily queries generated by users. This approach      eliminates the need for separate compute resources dedicated to data warehousing.

- Infrastructure Setup:
If the existing VM instance has sufficient capacity, the workflow orchestrator can be deployed directly on it. Otherwise, a lightweight GC e2-micro instance (approximately $30 per month) will host all pipeline-dependent tools and orchestration services.

- Data Warehousing and Query Costs:
Google BigQuery will be used for dashboard analytics, while all downstream transformations and queries will be executed in dbt within Cloud SQL, incurring no additional query costs.

BigQuery storage costs approximately $0.02 per GB per month.

A dataset of around 50 million rows is expected to occupy roughly 20 GB, resulting in a storage cost of $0.40 per month.

Query costs are $0.006 per GB processed (or $0.000006 per MB). With about 10 dashboards sourcing from well-optimized, normalized tables, the total monthly query cost is projected to stay within $5‚Äì$8.

Estimated Total Monthly Cost: approximately $40, covering compute, orchestration, storage, and query usage.



